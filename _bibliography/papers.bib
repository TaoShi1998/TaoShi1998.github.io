---
---

@string{aps = {American Physical Society,}}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{SSLCL_AAAI2024,
  abbr={SSLCL},
  title={SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations},
  author={Tao Shi, Xiao Liang, Yaoyuan Liang, Xinyi Tong, and Shao-Lun Huang},
  abstract={Emotion recognition in conversations (ERC) is a rapidly evolving task within the natural language processing commu- nity, which aims to detect the emotions expressed by speakers during a conversation. Recently, a growing number of ERC methods have focused on leveraging supervised contrastive learning (SCL) to enhance the robustness and generalizability of learned features. However, current SCL-based approaches in ERC are impeded by the constraint of large batch sizes and the lack of compatibility with most existing ERC models. To address these challenges, we propose an efficient and model- agnostic SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models with- out introducing any model-specific assumptions. Specifically, we introduce a novel perspective on utilizing label represen- tations by projecting discrete labels into dense embeddings through a shallow multilayer perceptron, and formulate the training objective to maximize the similarity between sample features and their corresponding ground-truth label embed- dings, while minimizing the similarity between sample fea- tures and label embeddings of disparate classes. Moreover, we innovatively adopt the Soft-HGR maximal correlation as a measure of similarity between sample features and label em- beddings, leading to significant performance improvements over conventional similarity measures. Additionally, multi- modal cues of utterances are effectively leveraged by SSLCL as data augmentations to boost model performances. Exten- sive experiments on two ERC benchmark datasets, IEMO- CAP and MELD, demonstrate the compatibility and superior- ity of our proposed SSLCL framework compared to existing state-of-the-art SCL methods.},
  journal={Under Review by AAAI 2024},
  code={https://github.com/TaoShi1998},
  doi={10.18653/v1/2023.acl-long.824},
  pdf={https://aclanthology.org/2023.acl-long.824/},
  selected={true}
}


@article{multiemo_acl2023,
  abbr={MultiEMO},
  title={MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations},
  author={Tao Shi and Shao-Lun Huang},
  abstract={Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a con- versation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the com- plementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correla- tions and mapping relationships across differ- ent modalities. Furthermore, existing state-of- the-art ERC models have difficulty classifying minority and semantically similar emotion cate- gories. To address these challenges, we propose a novel attention-based correlation-aware mul- timodal fusion framework named MultiEMO, which effectively integrates multimodal cues by capturing cross-modal mapping relation- ships across textual, audio and visual modal- ities based on bidirectional multi-head cross- attention layers. The difficulty of recognizing minority and semantically hard-to-distinguish emotion classes is alleviated by our proposed Sample-Weighted Focal Contrastive (SWFC) loss. Extensive experiments on two benchmark ERC datasets demonstrate that our MultiEMO framework consistently outperforms existing state-of-the-art approaches in all emotion cat- egories on both datasets, the improvements in minority and semantically similar emotions are especially significant.},
  journal={ACL 2023},
  code={https://github.com/TaoShi1998},
  doi={10.18653/v1/2023.acl-long.824},
  pdf={https://aclanthology.org/2023.acl-long.824/},
  selected={true}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations},
  author={Tao Shi and Shao-Lun Huang},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}




@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schr√∂dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}
