---
---

@string{aps = {American Physical Society,}}

@book{einstein1956investigations,
  bibtex_show={true},
  title={Investigations on the Theory of the Brownian Movement},
  author={Einstein, Albert},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1950meaning,
  abbr={AJP},
  bibtex_show={true},
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{SSLCL_AAAI2024,
  abbr={Under Review at TACL},
  title={SSLCL: An Efficient Model-Agnostic Supervised Contrastive Learning Framework for Emotion Recognition in Conversations},
  author={Shi, Tao and Liang, Xiao and Liang, Yaoyuan and Tong, Xinyi and Huang, Shao-Lun},
  abstract={Emotion recognition in conversations (ERC) is a rapidly evolving task within the natural language processing community, which aims to detect the emotions expressed by speakers during a conversation. Recently, a growing number of ERC methods have focused on leveraging supervised contrastive learning (SCL) to enhance the robustness and generalizability of learned features. However, current SCL-based approaches in ERC are impeded by the constraint of large batch sizes and the lack of compatibility with most existing ERC models. To address these challenges, we propose an efficient and model-agnostic SCL framework named Supervised Sample-Label Contrastive Learning with Soft-HGR Maximal Correlation (SSLCL), which eliminates the need for a large batch size and can be seamlessly integrated with existing ERC models without introducing any model-specific assumptions. Specifically, we introduce a novel perspective on utilizing label representations by projecting discrete labels into dense embeddings through a shallow multilayer perceptron, and formulate the training objective to maximize the similarity between sample features and their corresponding ground-truth label embeddings, while minimizing the similarity between sample features and label embeddings of disparate classes. Moreover, we innovatively adopt the Soft-HGR maximal correlation as a measure of similarity between sample features and label embeddings, leading to significant performance improvements over conventional similarity measures. Additionally, multimodal cues of utterances are effectively leveraged by SSLCL as data augmentations to boost model performances. Extensive experiments on two ERC benchmark datasets, IEMOCAP and MELD, demonstrate the compatibility and superiority of our proposed SSLCL framework compared to existing state-of-the-art SCL methods.},
  year={2024},
  code={https://github.com/TaoShi1998/SSLCL},
  pdf={https://arxiv.org/pdf/2310.16676.pdf},
  selected={true}
}

@article{DiffusionVG,
  abbr={ICME 2024},
  title={Exploring Iterative Refinement with Diffusion Models for Video Grounding},
  author={Liang, Xiao and Shi, Tao and Liang, Yaoyuan and Tao, Te and Huang, Shao-Lun},
  abstract={Video grounding aims to localize the target moment in an untrimmed video corresponding to a given sentence query. Existing methods typically select the best prediction from a set of predefined proposals or directly regress the target span in a single-shot manner, resulting in the absence of a systematical prediction refinement process. In this paper, we propose DiffusionVG, a novel framework with diffusion models that formulates video grounding as a conditional generation task, where the target span is generated from Gaussian noise inputs and interatively refined in the reverse diffusion process. During training, DiffusionVG progressively adds noise to the target span with a fixed forward diffusion process and learns to recover the target span in the reverse diffusion process. In inference, DiffusionVG can generate the target span from Gaussian noise inputs by the learned reverse diffusion process conditioned on the video-sentence representations. Our DiffusionVG follows the encoder-decoder architecture, which firstly encodes the video-sentence features and iteratively denoises the predicted spans in its specialized span refining decoder. Without bells and whistles, our DiffusionVG demonstrates competitive or even superior performance compared to existing well-crafted models on mainstream Charades-STA and ActivityNet Captions benchmarks.},
  year={2024},
  code={https://github.com/mastervito/diffusionvg},
  pdf={https://arxiv.org/pdf/2310.17189.pdf},
  selected={true}
}

@article{multiemo_acl2023,
  abbr={ACL 2023},
  title={MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations},
  author={Tao Shi and Shao-Lun Huang},
  abstract={Emotion Recognition in Conversations (ERC) is an increasingly popular task in the Natural Language Processing community, which seeks to achieve accurate emotion classifications of utterances expressed by speakers during a conversation. Most existing approaches focus on modeling speaker and contextual information based on the textual modality, while the complementarity of multimodal information has not been well leveraged, few current methods have sufficiently captured the complex correlations and mapping relationships across different modalities. Furthermore, existing state-of-the-art ERC models have difficulty classifying minority and semantically similar emotion categories. To address these challenges, we propose a novel attention-based correlation-aware multimodal fusion framework named MultiEMO, which effectively integrates multimodal cues by capturing cross-modal mapping relationships across textual, audio and visual modalities based on bidirectional multi-head cross-attention layers. The difficulty of recognizing minority and semantically hard-to-distinguish emotion classes is alleviated by our proposed Sample-Weighted Focal Contrastive (SWFC) loss. Extensive experiments on two benchmark ERC datasets demonstrate that our MultiEMO framework consistently outperforms existing state-of-the-art approaches in all emotion categories on both datasets, the improvements in minority and semantically similar emotions are especially significant.},
  year={2023},
  code={https://github.com/TaoShi1998/MultiEMO-ACL2023},
  doi={10.18653/v1/2023.acl-long.824},
  pdf={https://aclanthology.org/2023.acl-long.824/},
  selected={true}
}

@article{autoCODES_2024,
  abbr={Under Review at IEEE-ITW'24},
  title={A Non-asymptotic Framework for Characterizing Dependency Structures in Multimodal Learning},
  author={Wang, Weida and Shi, Tao and Liang, Yaoyuan and Tong, Xinyi and Huang, Shao-Lun},
  abstract={Dependency structures between modalities have been utilized explicitly and implicitly in multimodal learning to enhance classification performance, particularly when the training samples are insufficient. Recent efforts have concentrated on developing mathematical frameworks utilizing conditional dependency structures, but the non-asymptotic relations between the training sample size and various structures are not sufficiently addressed. To address this issue, we propose a mathematical framework that can be utilized to characterize conditional dependency structures in analytic ways. It provides an explicit description of the sample size in learning various structures in a non-asymptotic regime. Additionally, it demonstrates how task complexity and a fitness evaluation of conditional dependence structures affect the results. Furthermore, we develop an autonomously updated coefficient algorithm auto-CODES based on the theoretical framework and conduct experiments on multimodal emotion recognition tasks using the MELD dataset. The experimental results validate our theory and show the effectiveness of the proposed algorithm. },
  year={2023},
  code={https://github.com/TaoShi1998/auto-CODES},
  pdf={/assets/pdf/CV.pdf},
  selected={true}
}

@book{einstein1956investigations,
  bibtex_show={true},
  title={MultiEMO: An Attention-Based Correlation-Aware Multimodal Fusion Framework for Emotion Recognition in Conversations},
  author={Tao Shi and Shao-Lun Huang},
  year={1956},
  publisher={Courier Corporation},
  preview={brownian-motion.gif}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schr√∂dinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif}
}
